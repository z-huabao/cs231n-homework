{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Networks\n",
    "So far we have worked with deep fully-connected networks, using them to explore different optimization strategies and network architectures. Fully-connected networks are a good testbed for experimentation because they are very computationally efficient, but in practice all state-of-the-art results use convolutional networks instead.\n",
    "\n",
    "First you will implement several layer types that are used in convolutional networks. You will then use these layers to train a convolutional network on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import setuptools\n",
    "%load_ext Cython\n",
    "# As usual, a bit of setup\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.cnn import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient_array, eval_numerical_gradient\n",
    "from cs231n.layers import *\n",
    "from cs231n.fast_layers import *\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000L, 3L, 32L, 32L)\n",
      "X_train:  (49000L, 3L, 32L, 32L)\n",
      "X_test:  (1000L, 3L, 32L, 32L)\n",
      "y_val:  (1000L,)\n",
      "y_train:  (49000L,)\n",
      "y_test:  (1000L,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive forward pass\n",
    "The core of a convolutional network is the convolution operation. In the file `cs231n/layers.py`, implement the forward pass for the convolution layer in the function `conv_forward_naive`. \n",
    "\n",
    "You don't have to worry too much about efficiency at this point; just write the code in whatever way you find most clear.\n",
    "\n",
    "You can test your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_vector\n",
      "difference:  2.21214765759e-08\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layers import conv_forward_vector\n",
    "x_shape = (2, 3, 4, 4)\n",
    "w_shape = (3, 3, 4, 4)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=3)\n",
    "\n",
    "conv_param = {'stride': 2, 'pad': 1}\n",
    "out, _ = conv_forward_vector(x, w, b, conv_param)\n",
    "correct_out = np.array([[[[[-0.08759809, -0.10987781],\n",
    "                           [-0.18387192, -0.2109216 ]],\n",
    "                          [[ 0.21027089,  0.21661097],\n",
    "                           [ 0.22847626,  0.23004637]],\n",
    "                          [[ 0.50813986,  0.54309974],\n",
    "                           [ 0.64082444,  0.67101435]]],\n",
    "                         [[[-0.98053589, -1.03143541],\n",
    "                           [-1.19128892, -1.24695841]],\n",
    "                          [[ 0.69108355,  0.66880383],\n",
    "                           [ 0.59480972,  0.56776003]],\n",
    "                          [[ 2.36270298,  2.36904306],\n",
    "                           [ 2.38090835,  2.38247847]]]]])\n",
    "\n",
    "# Compare your output to ours; difference should be around 1e-8\n",
    "print 'Testing conv_forward_vector'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aside: Image processing via convolutions\n",
    "\n",
    "As fun way to both check your implementation and gain a better understanding of the type of operation that convolutional layers can perform, we will set up an input containing two images and manually set up filters that perform common image processing operations (grayscale conversion and edge detection). The convolution forward pass will apply these operations to each of the input images. We can then visualize the results as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread, imresize\n",
    "\n",
    "kitten, puppy = imread('kitten.jpg'), imread('puppy.jpg')\n",
    "# kitten is wide, and puppy is already square\n",
    "d = kitten.shape[1] - kitten.shape[0]\n",
    "kitten_cropped = kitten[:, d/2:-d/2, :]\n",
    "\n",
    "img_size = 200   # Make this smaller if it runs too slow\n",
    "x = np.zeros((2, 3, img_size, img_size))\n",
    "x[0, :, :, :] = imresize(puppy, (img_size, img_size)).transpose((2, 0, 1))\n",
    "x[1, :, :, :] = imresize(kitten_cropped, (img_size, img_size)).transpose((2, 0, 1))\n",
    "\n",
    "# Set up a convolutional weights holding 2 filters, each 3x3\n",
    "w = np.zeros((2, 3, 3, 3))\n",
    "\n",
    "# The first filter converts the image to grayscale.\n",
    "# Set up the red, green, and blue channels of the filter.\n",
    "w[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]\n",
    "w[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]\n",
    "w[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]\n",
    "\n",
    "# Second filter detects horizontal edges in the blue channel.\n",
    "w[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]\n",
    "\n",
    "# Vector of biases. We don't need any bias for the grayscale\n",
    "# filter, but for the edge detection filter we want to add 128\n",
    "# to each output so that nothing is negative.\n",
    "b = np.array([0, 128])\n",
    "\n",
    "# Compute the result of convolving each input in x with each filter in w,\n",
    "# offsetting by b, and storing the results in out.\n",
    "out, _ = conv_forward_naive(x, w, b, {'stride': 1, 'pad': 1})\n",
    "\n",
    "def imshow_noax(img, normalize=True):\n",
    "    \"\"\" Tiny helper to show images as uint8 and remove axis labels \"\"\"\n",
    "    if normalize:\n",
    "        img_max, img_min = np.max(img), np.min(img)\n",
    "        img = 255.0 * (img - img_min) / (img_max - img_min)\n",
    "    plt.imshow(img.astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "# Show the original images and the results of the conv operation\n",
    "plt.subplot(2, 3, 1)\n",
    "imshow_noax(puppy, normalize=False)\n",
    "plt.title('Original image')\n",
    "plt.subplot(2, 3, 2)\n",
    "imshow_noax(out[0, 0])\n",
    "plt.title('Grayscale')\n",
    "plt.subplot(2, 3, 3)\n",
    "imshow_noax(out[0, 1])\n",
    "plt.title('Edges')\n",
    "plt.subplot(2, 3, 4)\n",
    "imshow_noax(kitten_cropped, normalize=False)\n",
    "plt.subplot(2, 3, 5)\n",
    "imshow_noax(out[1, 0])\n",
    "plt.subplot(2, 3, 6)\n",
    "imshow_noax(out[1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution: Naive backward pass\n",
    "Implement the backward pass for the convolution operation in the function `conv_backward_naive` in the file `cs231n/layers.py`. Again, you don't need to worry too much about computational efficiency.\n",
    "\n",
    "When you are done, run the following to check your backward pass with a numeric gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_backward_vector function\n",
      "dx error:  2.09063015044e-09\n",
      "dw error:  6.43960501476e-10\n",
      "db error:  3.47485310018e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(4, 3, 5, 5)\n",
    "w = np.random.randn(2, 3, 3, 3)\n",
    "b = np.random.randn(2,)\n",
    "dout = np.random.randn(4, 2, 5, 5)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_forward_naive(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_forward_naive(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_forward_naive(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "out, cache = conv_forward_vector(x, w, b, conv_param)\n",
    "dx, dw, db = conv_backward_vector(dout, cache)\n",
    "\n",
    "# Your errors should be around 1e-9'\n",
    "print 'Testing conv_backward_vector function'\n",
    "print 'dx error: ', rel_error(dx, dx_num)\n",
    "print 'dw error: ', rel_error(dw, dw_num)\n",
    "print 'db error: ', rel_error(db, db_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive forward\n",
    "Implement the forward pass for the max-pooling operation in the function `max_pool_forward_naive` in the file `cs231n/layers.py`. Again, don't worry too much about computational efficiency.\n",
    "\n",
    "Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_forward_naive function:\n",
      "difference:  4.16666651573e-08\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layers import *\n",
    "x_shape = (2, 3, 4, 4)\n",
    "x = np.linspace(-0.3, 0.4, num=np.prod(x_shape)).reshape(x_shape)\n",
    "pool_param = {'pool_width': 2, 'pool_height': 2, 'stride': 2}\n",
    "\n",
    "out, cache = max_pool_forward_vector(x, pool_param)\n",
    "\n",
    "correct_out = np.array([[[[-0.26315789, -0.24842105],\n",
    "                          [-0.20421053, -0.18947368]],\n",
    "                         [[-0.14526316, -0.13052632],\n",
    "                          [-0.08631579, -0.07157895]],\n",
    "                         [[-0.02736842, -0.01263158],\n",
    "                          [ 0.03157895,  0.04631579]]],\n",
    "                        [[[ 0.09052632,  0.10526316],\n",
    "                          [ 0.14947368,  0.16421053]],\n",
    "                         [[ 0.20842105,  0.22315789],\n",
    "                          [ 0.26736842,  0.28210526]],\n",
    "                         [[ 0.32631579,  0.34105263],\n",
    "                          [ 0.38526316,  0.4       ]]]])\n",
    "\n",
    "# Compare your output with ours. Difference should be around 1e-8.\n",
    "print 'Testing max_pool_forward_naive function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max pooling: Naive backward\n",
    "Implement the backward pass for the max-pooling operation in the function `max_pool_backward_naive` in the file `cs231n/layers.py`. You don't need to worry about computational efficiency.\n",
    "\n",
    "Check your implementation with numeric gradient checking by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing max_pool_backward_naive function:\n",
      "dx error:  3.27564422241e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(3, 2, 8, 8)\n",
    "dout = np.random.randn(3, 2, 4, 4)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: max_pool_forward_vector(x, pool_param)[0], x, dout)\n",
    "\n",
    "out, cache = max_pool_forward_vector(x, pool_param)\n",
    "dx = max_pool_backward_vector(dout, cache)\n",
    "\n",
    "# Your error should be around 1e-12\n",
    "print 'Testing max_pool_backward_naive function:'\n",
    "print 'dx error: ', rel_error(dx, dx_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast layers\n",
    "Making convolution and pooling layers fast can be challenging. To spare you the pain, we've provided fast implementations of the forward and backward passes for convolution and pooling layers in the file `cs231n/fast_layers.py`.\n",
    "\n",
    "The fast convolution implementation depends on a Cython extension; to compile it you need to run the following from the `cs231n` directory:\n",
    "\n",
    "```bash\n",
    "python setup.py build_ext --inplace\n",
    "```\n",
    "\n",
    "The API for the fast versions of the convolution and pooling layers is exactly the same as the naive versions that you implemented above: the forward pass receives data, weights, and parameters and produces outputs and a cache object; the backward pass recieves upstream derivatives and the cache object and produces gradients with respect to the data and weights.\n",
    "\n",
    "**NOTE:** The fast implementation for pooling will only perform optimally if the pooling regions are non-overlapping and tile the input. If these conditions are not met then the fast pooling implementation will not be much faster than the naive implementation.\n",
    "\n",
    "You can compare the performance of the naive and fast versions of these layers by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_forward_fast:\n",
      "Naive: 0.194000s\n",
      "Fast:  0.253000s\n",
      "Speedup: 0.766798x\n",
      "Difference:  0.0\n",
      "\n",
      "Testing conv_backward_fast:\n",
      "Naive: 0.267000s\n",
      "Fast:  0.278000s\n",
      "Speedup: 0.960431x\n",
      "dx difference:  0.0\n",
      "dw difference:  0.0\n",
      "db difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "from cs231n.fast_layers import conv_forward_fast, conv_backward_fast\n",
    "from time import time\n",
    "\n",
    "x = np.random.randn(500, 3, 31, 31)\n",
    "w = np.random.randn(32, 3, 3, 3)\n",
    "b = np.random.randn(32,)\n",
    "dout = np.random.randn(500, 32, 31, 31)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = conv_forward_vector(x, w, b, conv_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = conv_forward_fast(x, w, b, conv_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing conv_forward_fast:'\n",
    "print 'Vector: %fs' % (t1 - t0)\n",
    "print 'Fast:  %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'Difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive, dw_naive, db_naive = conv_backward_vector(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast, dw_fast, db_fast = conv_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting conv_backward_fast:'\n",
    "print 'Vector: %fs' % (t1 - t0)\n",
    "print 'Fast:  %fs' % (t2 - t1)\n",
    "print 'Speedup: %fx' % ((t1 - t0) / (t2 - t1))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)\n",
    "print 'dw difference: ', rel_error(dw_naive, dw_fast)\n",
    "print 'db difference: ', rel_error(db_naive, db_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pool_forward:\n",
      "vector: 0.055000s\n",
      "fast: 0.061000s\n",
      "speedup: 1.109087x\n",
      "difference:  0.0\n",
      "\n",
      "Testing pool_backward:\n",
      "vector: 0.052000s\n",
      "fast: 0.143000s\n",
      "speedup: 2.750000x\n",
      "dx difference:  0.0\n"
     ]
    }
   ],
   "source": [
    "from cs231n.fast_layers import max_pool_forward_fast, max_pool_backward_fast\n",
    "from cs231n.layers import *\n",
    "x = np.random.randn(900, 3, 32, 32)\n",
    "dout = np.random.randn(900, 3, 16, 16)\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "t0 = time()\n",
    "out_naive, cache_naive = max_pool_forward_vector(x, pool_param)\n",
    "t1 = time()\n",
    "out_fast, cache_fast = max_pool_forward_fast(x, pool_param)\n",
    "t2 = time()\n",
    "\n",
    "print 'Testing pool_forward:'\n",
    "print 'vector: %fs' % (t1 - t0)\n",
    "print 'fast: %fs' % (t2 - t1)\n",
    "print 'speedup: %fx' % ((t2 - t1)/(t1 - t0))\n",
    "print 'difference: ', rel_error(out_naive, out_fast)\n",
    "\n",
    "t0 = time()\n",
    "dx_naive = max_pool_backward_vector(dout, cache_naive)\n",
    "t1 = time()\n",
    "dx_fast = max_pool_backward_fast(dout, cache_fast)\n",
    "t2 = time()\n",
    "\n",
    "print '\\nTesting pool_backward:'\n",
    "print 'vector: %fs' % (t1 - t0)\n",
    "print 'fast: %fs' % (t2 - t1)\n",
    "print 'speedup: %fx' % ((t2 - t1)/(t1 - t0))\n",
    "print 'dx difference: ', rel_error(dx_naive, dx_fast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional \"sandwich\" layers\n",
    "Previously we introduced the concept of \"sandwich\" layers that combine multiple operations into commonly used patterns. In the file `cs231n/layer_utils.py` you will find sandwich layers that implement a few commonly used patterns for convolutional networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu_pool\n",
      "dx error:  1.01709447452e-08\n",
      "dw error:  2.64964450777e-10\n",
      "db error:  4.79026210949e-10\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import conv_relu_pool_forward, conv_relu_pool_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 16, 16)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "pool_param = {'pool_height': 2, 'pool_width': 2, 'stride': 2}\n",
    "\n",
    "out, cache = conv_relu_pool_forward(x, w, b, conv_param, pool_param)\n",
    "dx, dw, db = conv_relu_pool_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_pool_forward(x, w, b, conv_param, pool_param)[0], b, dout)\n",
    "\n",
    "print 'Testing conv_relu_pool'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conv_relu:\n",
      "dx error:  1.02198641762e-08\n",
      "dw error:  1.18966090425e-09\n",
      "db error:  2.90428400421e-10\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import conv_relu_forward, conv_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 8, 8)\n",
    "w = np.random.randn(3, 3, 3, 3)\n",
    "b = np.random.randn(3,)\n",
    "dout = np.random.randn(2, 3, 8, 8)\n",
    "conv_param = {'stride': 1, 'pad': 1}\n",
    "\n",
    "out, cache = conv_relu_forward(x, w, b, conv_param)\n",
    "dx, dw, db = conv_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: conv_relu_forward(x, w, b, conv_param)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: conv_relu_forward(x, w, b, conv_param)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: conv_relu_forward(x, w, b, conv_param)[0], b, dout)\n",
    "\n",
    "print 'Testing conv_relu:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Three-layer ConvNet\n",
    "Now that you have implemented all the necessary layers, we can put them together into a simple convolutional network.\n",
    "\n",
    "Open the file `cs231n/cnn.py` and complete the implementation of the `ThreeLayerConvNet` class. Run the following cells to help you debug:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check loss\n",
    "After you build a new network, one of the first things you should do is sanity check the loss. When we use the softmax loss, we expect the loss for random weights (and no regularization) to be about `log(C)` for `C` classes. When we add regularization this should go up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.30258509299\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (32768,) (51200,32) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-288c4136cca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m'Initial loss (no regularization): '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\classifiers\\cnn.pyc\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     80\u001b[0m       \u001b[0mgamma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'beta1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m       \u001b[0mgamma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'beta2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m       \u001b[0mH1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_bn_relu_pool_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m       \u001b[0mH2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maff_bn_relu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mgamma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layer_utils.pyc\u001b[0m in \u001b[0;36mconv_bn_relu_pool_forward\u001b[0;34m(x, w, b, conv_param, gamma, beta, bn_param, pool_param)\u001b[0m\n\u001b[1;32m    118\u001b[0m   - cache: Object to give to the backward pass  \"\"\"\n\u001b[1;32m    119\u001b[0m   \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_forward_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m   \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspatial_batchnorm_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m   \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_forward_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layers.pyc\u001b[0m in \u001b[0;36mspatial_batchnorm_forward\u001b[0;34m(x, gamma, beta, bn_param)\u001b[0m\n\u001b[1;32m    316\u001b[0m   \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[1;31m# x_new = x.reshape(N, -1)另一种batchnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m   \u001b[0mout_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchnorm_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m   \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m   \u001b[1;31m# out = out_new.reshape(N, C, H, W)另一种batchnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layers.pyc\u001b[0m in \u001b[0;36mbatchnorm_forward\u001b[0;34m(x, gamma, beta, bn_param)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mVar\u001b[0m                       \u001b[1;31m# 方差变为 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[1;31m# print x.shape, gamma.shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbeta\u001b[0m            \u001b[1;31m# Covariate Shift\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m:\u001b[0m                                                      \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (32768,) (51200,32) "
     ]
    }
   ],
   "source": [
    "model = ThreeLayerConvNet()\n",
    "\n",
    "N = 50\n",
    "X = np.random.randn(N, 3, 32, 32)\n",
    "y = np.random.randint(10, size=N)\n",
    "\n",
    "print -np.log(0.1)\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (no regularization): ', loss\n",
    "model.reg = 0.05\n",
    "loss, grads = model.loss(X, y)\n",
    "print 'Initial loss (with regularization): ', loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient check\n",
    "After the loss looks reasonable, use numeric gradient checking to make sure that your backward pass is correct. When you use numeric gradient checking you should use a small amount of artifical data and a small number of neurons at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 max relative error: 3.159235e-08\n",
      "W2 max relative error: 4.059858e-07\n",
      "W3 max relative error: 3.880371e-03\n",
      "b1 max relative error: 2.076606e-10\n",
      "b2 max relative error: 1.052939e-09\n",
      "b3 max relative error: 1.316168e-04\n"
     ]
    }
   ],
   "source": [
    "num_inputs = 2\n",
    "input_dim = (3, 16, 16)\n",
    "reg = 0.0\n",
    "num_classes = 10\n",
    "X = np.random.randn(num_inputs, *input_dim)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=3, filter_size=3,\n",
    "                          input_dim=input_dim, hidden_dim=7,\n",
    "                          dtype=np.float64)\n",
    "loss, grads = model.loss(X, y)\n",
    "for param_name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, model.params[param_name], verbose=False, h=1e-6)\n",
    "    e = rel_error(param_grad_num, grads[param_name])\n",
    "    print '%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfit small data\n",
    "A nice trick is to train your model with just a few training samples. You should be able to overfit small datasets, which will result in very high training accuracy and comparatively low validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 3.278027\n",
      "(Epoch 0 / 20) train acc: 0.260000; val_acc: 0.085000\n",
      "(Epoch 1 / 20) train acc: 0.360000; val_acc: 0.125000\n",
      "(Epoch 2 / 20) train acc: 0.560000; val_acc: 0.140000\n",
      "(Epoch 3 / 20) train acc: 0.580000; val_acc: 0.172000\n",
      "(Epoch 4 / 20) train acc: 0.680000; val_acc: 0.197000\n",
      "(Epoch 5 / 20) train acc: 0.800000; val_acc: 0.178000\n",
      "(Iteration 11 / 40) loss: 0.802946\n",
      "(Epoch 6 / 20) train acc: 0.900000; val_acc: 0.155000\n",
      "(Epoch 7 / 20) train acc: 0.940000; val_acc: 0.127000\n",
      "(Epoch 8 / 20) train acc: 0.940000; val_acc: 0.122000\n",
      "(Epoch 9 / 20) train acc: 0.980000; val_acc: 0.131000\n",
      "(Epoch 10 / 20) train acc: 1.000000; val_acc: 0.156000\n",
      "(Iteration 21 / 40) loss: 0.119008\n",
      "(Epoch 11 / 20) train acc: 1.000000; val_acc: 0.171000\n",
      "(Epoch 12 / 20) train acc: 1.000000; val_acc: 0.174000\n",
      "(Epoch 13 / 20) train acc: 1.000000; val_acc: 0.177000\n",
      "(Epoch 14 / 20) train acc: 1.000000; val_acc: 0.172000\n",
      "(Epoch 15 / 20) train acc: 1.000000; val_acc: 0.169000\n",
      "(Iteration 31 / 40) loss: 0.070757\n",
      "(Epoch 16 / 20) train acc: 1.000000; val_acc: 0.160000\n",
      "(Epoch 17 / 20) train acc: 1.000000; val_acc: 0.156000\n",
      "(Epoch 18 / 20) train acc: 1.000000; val_acc: 0.155000\n",
      "(Epoch 19 / 20) train acc: 1.000000; val_acc: 0.154000\n",
      "(Epoch 20 / 20) train acc: 1.000000; val_acc: 0.154000\n"
     ]
    }
   ],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "model = ThreeLayerConvNet(num_filters=8, filter_size=3,\n",
    "                hidden_dim=150, use_batchnorm=False)\n",
    "\n",
    "solver = Solver(model, small_data, num_epochs=20, batch_size=25,\n",
    "                update_rule='adam', optim_config={'learning_rate': 1e-3,},\n",
    "                verbose=True, print_every=10)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the loss, training accuracy, and validation accuracy should show clear overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1QAAAKvCAYAAABtQ44zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYQ8iECSZ/woAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd0730b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(solver.train_acc_history, '-o')\n",
    "plt.plot(solver.val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the net\n",
    "By training the three-layer convolutional network for one epoch, you should achieve greater than 40% accuracy on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 1470) loss: 3.657345\n",
      "(Epoch 0 / 3) train acc: 0.112000; val_acc: 0.092000\n",
      "(Iteration 21 / 1470) loss: 2.261080\n",
      "(Iteration 41 / 1470) loss: 2.188160\n",
      "(Iteration 61 / 1470) loss: 2.113347\n",
      "(Iteration 81 / 1470) loss: 2.156948\n",
      "(Iteration 101 / 1470) loss: 1.892385\n",
      "(Iteration 121 / 1470) loss: 1.954005\n",
      "(Iteration 141 / 1470) loss: 1.696620\n",
      "(Iteration 161 / 1470) loss: 1.747917\n",
      "(Iteration 181 / 1470) loss: 1.720564\n",
      "(Iteration 201 / 1470) loss: 1.845853\n",
      "(Iteration 221 / 1470) loss: 1.782722\n",
      "(Iteration 241 / 1470) loss: 1.656714\n",
      "(Iteration 261 / 1470) loss: 1.608051\n",
      "(Iteration 281 / 1470) loss: 1.496331\n",
      "(Iteration 301 / 1470) loss: 1.706795\n",
      "(Iteration 321 / 1470) loss: 1.659658\n",
      "(Iteration 341 / 1470) loss: 1.564364\n",
      "(Iteration 361 / 1470) loss: 1.505851\n",
      "(Iteration 381 / 1470) loss: 1.622775\n",
      "(Iteration 401 / 1470) loss: 1.575250\n",
      "(Iteration 421 / 1470) loss: 1.503401\n",
      "(Iteration 441 / 1470) loss: 1.540202\n",
      "(Iteration 461 / 1470) loss: 1.273000\n",
      "(Iteration 481 / 1470) loss: 1.574185\n",
      "(Epoch 1 / 3) train acc: 0.495000; val_acc: 0.502000\n",
      "(Iteration 501 / 1470) loss: 1.461882\n",
      "(Iteration 521 / 1470) loss: 1.387746\n",
      "(Iteration 541 / 1470) loss: 1.316098\n",
      "(Iteration 561 / 1470) loss: 1.405949\n",
      "(Iteration 581 / 1470) loss: 1.576840\n",
      "(Iteration 601 / 1470) loss: 1.465248\n",
      "(Iteration 621 / 1470) loss: 1.394157\n",
      "(Iteration 641 / 1470) loss: 1.652701\n",
      "(Iteration 661 / 1470) loss: 1.523177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f922aaccbfa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                 verbose=True, print_every=20)\n\u001b[1;32m      7\u001b[0m \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\solver.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0miterations_per_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m       \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m       \u001b[1;31m# Maybe print training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\solver.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[1;31m# Make a minibatch of training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0mnum_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m     \u001b[0mbatch_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m       \u001b[1;31m#随机地获得小批量索引\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0mX_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "model = ThreeLayerConvNet(weight_scale=0.001, num_filters=32, filter_size=7, hidden_dim=100, \n",
    "                          use_batchnorm=False, reg=0.001)\n",
    "solver = Solver(model, data, num_epochs=3, batch_size=100,\n",
    "                update_rule='adam', optim_config={'learning_rate': 1e-3,},\n",
    "                verbose=True, print_every=20)\n",
    "t0 = time()\n",
    "solver.train()\n",
    "print time()-t0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Filters\n",
    "You can visualize the first-layer convolutional filters from the trained network by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGtCAYAAABOYZA0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNC4AgFdoXAAAr/w/JsfH2EepWSIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd06cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGtCAYAAABOYZA0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNC4AgFdoXAAAr/w/JsfH2EepWSIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd59d630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from cs231n.vis_utils import visualize_grid\n",
    "grid = visualize_grid(model.params['W1'].transpose(0, 2, 3, 1))\n",
    "plt.imshow(grid.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.gcf().set_size_inches(5, 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Batch Normalization\n",
    "We already saw that batch normalization is a very useful technique for training deep fully-connected networks. Batch normalization can also be used for convolutional networks, but we need to tweak it a bit; the modification will be called \"spatial batch normalization.\"\n",
    "\n",
    "Normally batch-normalization accepts inputs of shape `(N, D)` and produces outputs of shape `(N, D)`, where we normalize across the minibatch dimension `N`. For data coming from convolutional layers, batch normalization needs to accept inputs of shape `(N, C, H, W)` and produce outputs of shape `(N, C, H, W)` where the `N` dimension gives the minibatch size and the `(H, W)` dimensions give the spatial size of the feature map.\n",
    "\n",
    "If the feature map was produced using convolutions, then we expect the statistics of each feature channel to be relatively consistent both between different imagesand different locations within the same image. Therefore spatial batch normalization computes a mean and variance for each of the `C` feature channels by computing statistics over both the minibatch dimension `N` and the spatial dimensions `H` and `W`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial batch normalization: forward\n",
    "\n",
    "In the file `cs231n/layers.py`, implement the forward pass for spatial batch normalization in the function `spatial_batchnorm_forward`. Check your implementation by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before spatial batch normalization:\n",
      "  Shape:  (2L, 3L, 4L, 5L)\n",
      "  Means:  [ 8.71990832  9.5180591   9.02561116]\n",
      "  Stds:  [ 3.91402676  4.61652026  4.89431565]\n",
      "After spatial batch normalization:\n",
      "  Shape:  (2L, 3L, 4L, 5L)\n",
      "  Means:  [  2.18228213e-16  -2.10942375e-16  -3.66373598e-16]\n",
      "  Stds:  [ 1.  1.  1.]\n",
      "After spatial batch normalization (nontrivial gamma, beta):\n",
      "  Shape:  (2L, 3L, 4L, 5L)\n",
      "  Means:  [ 4.  4.  4.]\n",
      "  Stds:  [ 3.  3.  3.]\n"
     ]
    }
   ],
   "source": [
    "# Check the training-time forward pass by checking means and variances\n",
    "# of features both before and after spatial batch normalization\n",
    "\n",
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 4 * np.random.randn(N, C, H, W) + 10\n",
    "\n",
    "print 'Before spatial batch normalization:'\n",
    "print '  Shape: ', x.shape\n",
    "print '  Means: ', x.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', x.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to zero and stds close to one\n",
    "gamma, beta = np.ones(C), np.zeros(C)\n",
    "bn_param = {'mode': 'train'}\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization:'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))\n",
    "\n",
    "# Means should be close to beta and stds close to gamma\n",
    "gamma, beta = np.ones(C)*3, np.zeros(C)+4\n",
    "# gamma, beta = np.asarray([3, 4, 5]), np.asarray([6, 7, 8])\n",
    "\n",
    "out, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "print 'After spatial batch normalization (nontrivial gamma, beta):'\n",
    "print '  Shape: ', out.shape\n",
    "print '  Means: ', out.mean(axis=(0, 2, 3))\n",
    "print '  Stds: ', out.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After spatial batch normalization (test-time):\n",
      "  means:  [-0.03476356  0.0225672  -0.00210503  0.05177661]\n",
      "  stds:  [ 1.00962724  0.96529804  1.02196274  1.00024415]\n"
     ]
    }
   ],
   "source": [
    "# Check the test-time forward pass by running the training-time\n",
    "# forward pass many times to warm up the running averages, and then\n",
    "# checking the means and variances of activations after a test-time\n",
    "# forward pass.\n",
    "\n",
    "N, C, H, W = 10, 4, 11, 12\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "gamma = np.ones(C)\n",
    "beta = np.zeros(C)\n",
    "for t in xrange(50):\n",
    "  x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "  spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "bn_param['mode'] = 'test'\n",
    "x = 2.3 * np.random.randn(N, C, H, W) + 13\n",
    "a_norm, _ = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "\n",
    "# Means should be close to zero and stds close to one, but will be\n",
    "# noisier than training-time forward passes.\n",
    "print 'After spatial batch normalization (test-time):'\n",
    "print '  means: ', a_norm.mean(axis=(0, 2, 3))\n",
    "print '  stds: ', a_norm.std(axis=(0, 2, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial batch normalization: backward\n",
    "In the file `cs231n/layers.py`, implement the backward pass for spatial batch normalization in the function `spatial_batchnorm_backward`. Run the following to check your implementation using a numeric gradient check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dx error:  1.0\n",
      "dgamma error:  6.59871932905e-12\n",
      "dbeta error:  3.27571419736e-12\n"
     ]
    }
   ],
   "source": [
    "N, C, H, W = 2, 3, 4, 5\n",
    "x = 5 * np.random.randn(N, C, H, W) + 12\n",
    "gamma = np.random.randn(C)\n",
    "beta = np.random.randn(C)\n",
    "dout = np.random.randn(N, C, H, W)\n",
    "\n",
    "bn_param = {'mode': 'train'}\n",
    "fx = lambda x: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fg = lambda a: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "fb = lambda b: spatial_batchnorm_forward(x, gamma, beta, bn_param)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "da_num = eval_numerical_gradient_array(fg, gamma, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, beta, dout)\n",
    "\n",
    "_, cache = spatial_batchnorm_forward(x, gamma, beta, bn_param)\n",
    "dx, dgamma, dbeta = spatial_batchnorm_backward(dout, cache)\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dgamma error: ', rel_error(da_num, dgamma)\n",
    "print 'dbeta error: ', rel_error(db_num, dbeta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment!\n",
    "Experiment and try to get the best performance that you can on CIFAR-10 using a ConvNet. Here are some ideas to get you started:\n",
    "\n",
    "### Things you should try:\n",
    "- Filter size: Above we used 7x7; this makes pretty pictures but smaller filters may be more efficient\n",
    "- Number of filters: Above we used 32 filters. Do more or fewer do better?\n",
    "- Batch normalization: Try adding spatial batch normalization after convolution layers and vanilla batch normalization aafter affine layers. Do your networks train faster?\n",
    "- Network architecture: The network above has two layers of trainable parameters. Can you do better with a deeper network? You can implement alternative architectures in the file `cs231n/classifiers/convnet.py`. Some good architectures to try include:\n",
    "    - [conv-relu-pool]xN - conv - relu - [affine]xM - [softmax or SVM]\n",
    "    - [conv-relu-pool]XN - [affine]XM - [softmax or SVM]\n",
    "    - [conv-relu-conv-relu-pool]xN - [affine]xM - [softmax or SVM]\n",
    "\n",
    "### Tips for training\n",
    "For each network architecture that you try, you should tune the learning rate and regularization strength. When doing this there are a couple important things to keep in mind:\n",
    "\n",
    "- If the parameters are working well, you should see improvement within a few hundred iterations\n",
    "- Remember the course-to-fine approach for hyperparameter tuning: start by testing a large range of hyperparameters for just a few training iterations to find the combinations of parameters that are working at all.\n",
    "- Once you have found some sets of parameters that seem to work, search more finely around these parameters. You may need to train for more epochs.\n",
    "\n",
    "### Going above and beyond\n",
    "If you are feeling adventurous there are many other features you can implement to try and improve your performance. You are **not required** to implement any of these; however they would be good things to try for extra credit.\n",
    "\n",
    "- Alternative update steps: For the assignment we implemented SGD+momentum, RMSprop, and Adam; you could try alternatives like AdaGrad or AdaDelta.\n",
    "- Alternative activation functions such as leaky ReLU, parametric ReLU, or MaxOut.\n",
    "- Model ensembles\n",
    "- Data augmentation\n",
    "\n",
    "If you do decide to implement something extra, clearly describe it in the \"Extra Credit Description\" cell below.\n",
    "\n",
    "### What we expect\n",
    "At the very least, you should be able to train a ConvNet that gets at least 65% accuracy on the validation set. This is just a lower bound - if you are careful it should be possible to get accuracies much higher than that! Extra credit points will be awarded for particularly high-scoring models or unique approaches.\n",
    "\n",
    "You should use the space below to experiment and train your network. The final cell in this notebook should contain the training, validation, and test set accuracies for your final trained network. In this notebook you should also write an explanation of what you did, any additional features that you implemented, and any visualizations or graphs that you make in the process of training and evaluating your network.\n",
    "\n",
    "Have fun and happy training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.fliplr(data['X_train'].swapaxes(1,3)).swapaxes(1,3)\n",
    "data['X_train'] = np.vstack((data['X_train'], X))\n",
    "data['y_train'] = np.hstack((data['y_train'], data['y_train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 765) loss: 2.594561\n",
      "(Epoch 0 / 1) train acc: 0.170000; val_acc: 0.167000\n",
      "(Iteration 9 / 765) loss: 2.103553\n",
      "(Iteration 17 / 765) loss: 2.353864\n",
      "(Iteration 25 / 765) loss: 2.314984\n",
      "(Iteration 33 / 765) loss: 2.367498\n",
      "(Iteration 41 / 765) loss: 2.269581\n",
      "(Iteration 49 / 765) loss: 2.432433\n",
      "(Iteration 57 / 765) loss: 2.295667\n",
      "(Iteration 65 / 765) loss: 2.265839\n",
      "(Iteration 73 / 765) loss: 2.270158\n",
      "(Iteration 81 / 765) loss: 2.255180\n",
      "(Iteration 89 / 765) loss: 2.383008\n",
      "(Iteration 97 / 765) loss: 2.200105\n",
      "(Iteration 105 / 765) loss: 2.267525\n",
      "(Iteration 113 / 765) loss: 2.264254\n",
      "(Iteration 121 / 765) loss: 2.261012\n",
      "(Iteration 129 / 765) loss: 2.168741\n",
      "(Iteration 137 / 765) loss: 2.267332\n",
      "(Iteration 145 / 765) loss: 2.262686\n",
      "(Iteration 153 / 765) loss: 2.233029\n",
      "(Iteration 161 / 765) loss: 2.209565\n",
      "(Iteration 169 / 765) loss: 2.263227\n",
      "(Iteration 177 / 765) loss: 2.288539\n",
      "(Iteration 185 / 765) loss: 2.320896\n",
      "(Iteration 193 / 765) loss: 2.294565\n",
      "(Iteration 201 / 765) loss: 2.240618\n",
      "(Iteration 209 / 765) loss: 2.210724\n",
      "(Iteration 217 / 765) loss: 2.224869\n",
      "(Iteration 225 / 765) loss: 2.244044\n",
      "(Iteration 233 / 765) loss: 2.249550\n",
      "(Iteration 241 / 765) loss: 2.152482\n",
      "(Iteration 249 / 765) loss: 2.318384\n",
      "(Iteration 257 / 765) loss: 2.343847\n",
      "(Iteration 265 / 765) loss: 2.324184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-b83f7930ecca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mupdate_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m3e-3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 verbose=True, print_every=p)\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\solver.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mnum_iterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0miterations_per_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m       \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m       \u001b[1;31m# Maybe print training loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\solver.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[1;31m# Compute loss and gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[1;31m# learning_rate新算法： lr = (loss - min_loss) / (sum(dw))   , min_loss = 0.5*reg*sum(W**2)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\classifiers\\convnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    149\u001b[0m                     \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maff_bn_relu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 全连接\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                     \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_bn_relu_pool_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 卷积\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m                 \u001b[0mdW\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mW\u001b[0m  \u001b[1;31m# 加正则化梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'gamma%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'beta%d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layer_utils.pyc\u001b[0m in \u001b[0;36mconv_bn_relu_pool_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_pool_backward_victor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0mda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m   \u001b[0mda\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspatial_batchnorm_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m   \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_backward_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layers.py\u001b[0m in \u001b[0;36mspatial_batchnorm_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    645\u001b[0m   \u001b[0mdout_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m   \u001b[1;31m# dout_new = dout.reshape(N, -1)另一种batchnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m   \u001b[0mdx_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatchnorm_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout_new\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m   \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdx_new\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m   \u001b[1;31m# dx = dx_new.reshape(N, C, H, W)另一种batchnorm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layers.py\u001b[0m in \u001b[0;36mbatchnorm_backward\u001b[0;34m(dout, cache)\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0mdbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0mdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdout\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgamma\u001b[0m           \u001b[1;31m# H * gamma + beta = H~\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m   \u001b[0mdVar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mVar\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m   \u001b[1;31m# 好像是在批量方向加起来\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m   \u001b[0mdE1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mVar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m   \u001b[0mdE2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdVar\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mE\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a really good model on CIFAR-10\n",
    "from cs231n.classifiers.convnet import ConvNet\n",
    "\n",
    "# num_train, batch_size, n, p = 32, 64, 10, 1\n",
    "num_train, batch_size, n, p = -1, 128, 1, 8\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}# np.fliplr(X.swapaxes(1,3)).swapaxes(1,3)\n",
    "model = ConvNet(conv_sets=[(32,5,5,1),(32,3,3,1),(64,3,3,1),\n",
    "                           (64,3,3,1),(64,3,3,1),(128,3,3,1),\n",
    "                           (128,3,3,1),(128,3,3,1),(128,3,3,1)],\n",
    "                pool_params=[(1,1,1),(1,1,1),(3,3,2),\n",
    "                             (1,1,1),(1,1,1),(3,3,2),\n",
    "                             (1,1,1),(1,1,1),(3,3,2)],\n",
    "                aff_dim=[128], use_batchnorm=True, reg=0.0002, reset=True)\n",
    "solver = Solver(model, small_data, num_epochs=n, batch_size=batch_size,\n",
    "                update_rule='adam', optim_config={'learning_rate': 3e-3,},\n",
    "                verbose=True, print_every=p)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-673bb2551d32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m solver = Solver(model, data, num_epochs=1, batch_size=128,\n\u001b[1;32m      9\u001b[0m                 \u001b[0mupdate_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptim_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 verbose=True, print_every=50)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\solver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, data, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[1;31m#    self.X_train = data['X_val']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[1;31m#    self.y_train = data['y_val']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'X_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# num_train = 10000\n",
    "# small_data = {\n",
    "#   'X_train': data['X_train'][num_train:num_train+10000],\n",
    "#   'y_train': data['y_train'][num_train:num_train+10000],\n",
    "#   'X_val': data['X_val'],\n",
    "#   'y_val': data['y_val'],\n",
    "# }\n",
    "solver = Solver(model, data, num_epochs=1, batch_size=128,\n",
    "                update_rule='adam', optim_config={'learning_rate': 1e-3,},\n",
    "                verbose=True, print_every=50)\n",
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-98cdc17ba18f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_val'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0my_val_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_val_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtest_acc\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0my_test_pred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\classifiers\\convnet.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     H, cache[j] = conv_bn_relu_pool_forward(H, W, b, conv_params[j],\n\u001b[0;32m--> 120\u001b[0;31m                                                            gamma, beta, bn_params[j], pool_params[j])  # 卷积\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                     \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maff_bn_relu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 全连接\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\layer_utils.pyc\u001b[0m in \u001b[0;36mconv_bn_relu_pool_forward\u001b[0;34m(x, w, b, conv_param, gamma, beta, bn_param, pool_param)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[1;33m-\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOutput\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpooling\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   - cache: Object to give to the backward pass  \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv_forward_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconv_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m   \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspatial_batchnorm_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbn_param\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m   \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ZHB\\assignment2\\cs231n\\fast_layers.py\u001b[0m in \u001b[0;36mconv_forward_victor\u001b[0;34m(x, w, b, conv_param)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw_flash\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mWW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mx_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh_flash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_flash\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0mx_pad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m   \u001b[0mx_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.astype(np.float32) # 将x变换后的形状，预分配内存\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = model\n",
    "val_acc, test_acc = 0, 0\n",
    "for i in xrange(10):\n",
    "    j = i*100\n",
    "    X_test = data['X_test'][j:j+100]\n",
    "    X_val = data['X_val'][j:j+100]\n",
    "    y_test = data['y_test'][j:j+100]\n",
    "    y_val = data['y_val'][j:j+100]\n",
    "    y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "    y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "    val_acc += (y_val_pred == y_val).mean()\n",
    "    test_acc += (y_test_pred == y_test).mean()\n",
    "print 'Validation set accuracy: ', val_acc/10\n",
    "print 'Test set accuracy: ', test_acc/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "io.savemat('C:\\\\Users\\\\ZHB\\\\assignment2\\\\cs231n\\\\classifiers\\\\best_params.mat', model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit Description\n",
    "If you implement any additional features for extra credit, clearly describe them here with pointers to any code in this or other files if applicable."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
